"""
Pump.fun Movers Scraper - Phase 57
Scrapes low MC movers directly from pump.fun website using residential proxy.
"""
import requests
import os
import re
import json
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

def get_session():
    """Get a requests session with residential proxy if available."""
    session = requests.Session()
    proxy_url = os.getenv('RESIDENTIAL_PROXY')
    if proxy_url:
        session.proxies = {'http': proxy_url, 'https': proxy_url}
        print("ðŸŒ Using residential proxy")
    else:
        print("âš ï¸ No proxy configured")
    return session

def scrape_pump_board():
    """Scrape the main pump.fun board for tokens."""
    session = get_session()
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    print("\n" + "="*80)
    print("ðŸ” SCRAPING PUMP.FUN BOARD")
    print("="*80)
    
    try:
        # Try the main board page
        resp = session.get('https://pump.fun/board', headers=headers, timeout=20)
        print(f"Status: {resp.status_code}")
        print(f"Content-Type: {resp.headers.get('content-type', 'unknown')}")
        print(f"Content Length: {len(resp.text)} chars")
        
        if resp.status_code == 200:
            # Check if we got HTML
            if 'text/html' in resp.headers.get('content-type', ''):
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # Print page title
                title = soup.find('title')
                print(f"Page Title: {title.text if title else 'None'}")
                
                # Look for token cards/items
                # pump.fun likely uses React, so look for data in script tags
                scripts = soup.find_all('script')
                print(f"\nFound {len(scripts)} script tags")
                
                found_tokens = False
                
                # Search ALL scripts for token-related JSON
                for idx, script in enumerate(scripts):
                    script_text = script.string or ''
                    
                    # Look for __NEXT_DATA__ 
                    if script.get('id') == '__NEXT_DATA__':
                        print(f"\nâœ… Found __NEXT_DATA__ at script #{idx}")
                        try:
                            data = json.loads(script_text)
                            print(f"Keys: {list(data.keys())[:5]}")
                            # Deep search for tokens
                            found_tokens = search_for_tokens(data, "NEXT_DATA")
                        except:
                            pass
                    
                    # Look for any script containing token keywords
                    keywords = ['usd_market_cap', 'market_cap', '"mint":', '"symbol":', 'virtualSolReserves']
                    for kw in keywords:
                        if kw in script_text:
                            print(f"\nðŸŽ¯ Found '{kw}' in script #{idx} ({len(script_text)} chars)")
                            
                            # Try to extract JSON arrays
                            for match in re.finditer(r'(\[[\s\S]*?\])', script_text):
                                try:
                                    arr = json.loads(match.group(1))
                                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], dict):
                                        if any(k in arr[0] for k in ['mint', 'symbol', 'market_cap', 'usd_market_cap']):
                                            print(f"  âœ… Found token array with {len(arr)} items!")
                                            for i, t in enumerate(arr[:8]):
                                                symbol = t.get('symbol', t.get('name', '?'))
                                                mc = t.get('usd_market_cap', t.get('market_cap', 0))
                                                mint = t.get('mint', '?')[:16]
                                                print(f"    {i+1}. {symbol:12} | MC: ${mc:>12,.0f} | {mint}...")
                                            found_tokens = True
                                            break
                                except:
                                    pass
                            
                            # Also try extracting JSON objects
                            for match in re.finditer(r'(\{[^\{\}]{100,10000}\})', script_text):
                                try:
                                    obj = json.loads(match.group(1))
                                    if 'mint' in obj and ('symbol' in obj or 'market_cap' in obj):
                                        print(f"  Found single token: {obj.get('symbol', '?')}")
                                except:
                                    pass
                            break  # Only report once per script
                
                if not found_tokens:
                    print("\nâš ï¸ No embedded token data found in scripts")
                    print("Checking raw HTML for clues...")
                    # Look for data attributes
                    html_preview = resp.text[:5000]
                    if 'data-' in html_preview:
                        attrs = re.findall(r'data-[\w-]+', html_preview)
                        print(f"Data attributes found: {set(attrs)}")
                
                # Check for specific elements that might contain tokens
                token_els = soup.find_all(attrs={'data-token': True}) or soup.find_all(class_=re.compile(r'token|card|coin', re.I))
                if token_els:
                    print(f"\nâœ… Found {len(token_els)} token elements by class/attr!")
                    
            else:
                print(f"Response preview: {resp.text[:500]}")
                
        else:
            print(f"Error: {resp.text[:500]}")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


def scrape_pump_advanced():
    """
    Advanced scraping using curl_cffi for TLS fingerprint impersonation.
    This better mimics a real browser and can bypass some Cloudflare checks.
    """
    try:
        from curl_cffi import requests as curl_requests
        print("\n" + "="*80)
        print("ðŸ” ADVANCED SCRAPE WITH CURL_CFFI (Chrome Impersonation)")
        print("="*80)
        
        proxy_url = os.getenv('RESIDENTIAL_PROXY')
        proxies = {'http': proxy_url, 'https': proxy_url} if proxy_url else None
        
        # Use Chrome impersonation
        resp = curl_requests.get(
            'https://pump.fun/board',
            impersonate="chrome120",
            proxies=proxies,
            timeout=20
        )
        
        print(f"Status: {resp.status_code}")
        print(f"Content Length: {len(resp.text)} chars")
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            title = soup.find('title')
            print(f"Page Title: {title.text if title else 'None'}")
            
            # Diagnostic: Show first few scripts
            scripts = soup.find_all('script')
            print(f"\nðŸ“‹ SCRIPT DIAGNOSTIC ({len(scripts)} total)")
            
            for idx, script in enumerate(scripts[:20]):
                src = script.get('src', '')
                content = (script.string or '')[:200]
                
                if src:
                    print(f"\n  #{idx}: SRC={src[:80]}")
                elif content:
                    # Show preview of inline scripts
                    preview = content.replace('\n', ' ').strip()
                    print(f"\n  #{idx}: INLINE ({len(script.string or '')} chars)")
                    print(f"       Preview: {preview[:100]}...")
                    
                    # Look for API/WebSocket URLs
                    if 'wss://' in (script.string or ''):
                        ws_matches = re.findall(r'wss://[^\s"\']+', script.string)
                        print(f"       ðŸ”Œ WebSocket URLs: {ws_matches}")
                    
                    if 'api' in (script.string or '').lower():
                        api_matches = re.findall(r'https://[^\s"\']*api[^\s"\']*', script.string, re.I)
                        if api_matches:
                            print(f"       ðŸŒ API URLs: {api_matches[:3]}")
                    
    except ImportError:
        print("\nâš ï¸ curl_cffi not installed. Run: pip3 install curl_cffi")
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


def search_for_tokens(data, source=""):
    """Recursively search for token arrays in nested data."""
    if isinstance(data, list):
        if len(data) > 0 and isinstance(data[0], dict):
            if any(k in data[0] for k in ['mint', 'symbol', 'market_cap', 'usd_market_cap']):
                print(f"\nðŸŽ¯ Found {len(data)} tokens in {source}!")
                for i, t in enumerate(data[:8]):
                    symbol = t.get('symbol', t.get('name', '?'))
                    mc = t.get('usd_market_cap', t.get('market_cap', 0))
                    mint = t.get('mint', '?')[:16]
                    print(f"    {i+1}. {symbol:12} | MC: ${mc:>12,.0f} | {mint}...")
                return True
    elif isinstance(data, dict):
        for key, value in data.items():
            if search_for_tokens(value, f"{source}.{key}"):
                return True
    return False


if __name__ == "__main__":
    scrape_pump_board()
    scrape_pump_advanced()
